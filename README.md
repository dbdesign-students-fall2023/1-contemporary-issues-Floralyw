# Samsung Bans Employees' AI Use After Spotting ChatGPT Data Leak
In this document, I discuss two articles about the issue that Samsung employees inadvertently leaked sensitive corporate data to ChatGPT. And a number of firms started to block access to ChatGPT. I present and analyze two opposing views on this matter.

## Article 1
In AdGuard's [Can’t take it back: Samsung ChatGPT fiasco highlights generative AI privacy woes](https://adguard.com/en/blog/samsung-chatgpt-leak-privacy.html), the editor Ekaterina Kachalova presented the information that besides Samsung, companies including Amazon and Walmart had also raised concerns about ChatGPT's security risks, and numerous financial institutions, including Bank of America, JPMorgan Chase, and Goldman Sachs, banned its use. As there are no methods for ChatGPT to "unlearn" data, if a company's employees share confidential information with ChatGPT, it will be used to train the algorithm and someone else can potentially get that data. Thus, the editor argues that "it's better not to trust AI helpers" due to potential accidental data leaks.

I think it is reasonable for companies, especially financial institutions and tech firms, to restrict the use of ChatGPT from the workplace to strictly protect customers' personal information and the company's core technology. However, I think it's unlikely to ban the usage of ChatGPT and other AI chatbots forever. It is undeniable that ChatGPT can help employees generate new ideas, fix coding errors, and improve overall working efficiency. Therefore, I believe that it is more important to foster a mindset of data security and privacy among the employees to protect the company's privacy, instead of completely forbidding ChatGPT usage.

## Article 2
Contrary to the claims made in AdGuard's piece on the issue, Journalist Vilius Petkauskas from Cybernews [Lessons learned from ChatGPT’s Samsung leak](https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/) discusses two other ways besides the prohibition of ChatGPT. According to Tyler Young, the chief information security officer at BigID, this phenomenon, termed "conversational AI leak", denotes the inadvertent exposure of crucial data via Large Language Models (LLMs), which is facilitated by the nature and structure of ChatGPT. One of the methods to prevent ChatGPT from divulging sensitive learned data is to limit the output of these chatbots, controlling what they can and cannot provide to users. The other way is to control data inputs and limit the number of employees who can get access to these AI tools.

I am prone to the second way of limiting input. First of all, output control is still a problematic technology and needs more research. Second, I really agree with the CEO of Code42 Joe Payne's view that "ChatGPT is just one of many generative AI tools that will be introduced to the workplace in the coming years." Thus, simply banning such tools isn't a viable long-term strategy, given their impending proliferation and unimaginative capacity. In my opinion, a comprehensive approach encompassing user education, strict access controls, and regular audits should be considered.

Taking a broader view, the ongoing debate over the security of AI platforms like ChatGPT highlights the challenges and importance of ensuring data privacy in the digital age. Both the platform developers and organizations using these tools need to undertake the responsibilities of data security.

